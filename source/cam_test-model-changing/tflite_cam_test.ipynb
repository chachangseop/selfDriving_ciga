{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd459f6f-89ec-4c95-88ab-af56437cf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import timeit\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# TFLite 모델 파일 경로\n",
    "model_path = 'C:/Users/USER/Documents/vision-AI/project/7899/models/saved_model/best_float32.tflite'\n",
    "\n",
    "# TFLite 모델을 로드합니다.\n",
    "interpreter = tf.lite.Interpreter(model_path=model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# 입력 텐서 정보 가져오기\n",
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "# 출력 텐서 정보 가져오기\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# 입력 텐서의 모양 및 데이터 유형\n",
    "input_shape = input_details[0]['shape']\n",
    "input_dtype = input_details[0]['dtype']\n",
    "\n",
    "# 출력 텐서의 모양 및 데이터 유형\n",
    "output_shape = output_details[0]['shape']\n",
    "output_dtype = output_details[0]['dtype']\n",
    "\n",
    "\n",
    "#웹캠 초기화\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 알고리즘 시작 시점\n",
    "    start_t = timeit.default_timer()\n",
    "\n",
    "    annotated_frame = frame  # 기본적으로 원본 프레임으로 초기화\n",
    "\n",
    "    # 모델을 사용하여 프레임에서 객체 감지\n",
    "    # 입력 데이터 크기를 [1, 3, 640, 640]로 변경\n",
    "    input_data = np.zeros((1,640, 640, 3), dtype=np.uint8)\n",
    "    for i in range(480):\n",
    "        for j in range(640):\n",
    "            for rgb in range(3):\n",
    "                input_data[0][i][j][rgb] = frame[i][j][rgb]\n",
    "\n",
    "    input_data = input_data.astype(np.float32)  # FLOAT32로 변환\n",
    "\n",
    "    # 입력 텐서에 데이터를 설정합니다.\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    \n",
    "    # 추론을 실행합니다.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # 출력 텐서에서 결과 가져오기\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "\n",
    "    # 출력을 해석합니다.\n",
    "    # (1, 6, 8400) -> (batch_size, attributes, num_detections)\n",
    "    detections = output_data[0]  # (6, 8400)\n",
    "\n",
    "    print(detections[0])\n",
    "\n",
    "    for detection in detections.T:  # Transpose to iterate over detections\n",
    "        score = detection[4]  # 예시로 5번째 요소가 신뢰도(score)라고 가정\n",
    "        if score > 0.8:  # 임계값(Threshold) 설정\n",
    "            x, y, w, h = detection[:4]\n",
    "            class_id = detection[5]\n",
    "            # 바운딩 박스 그리기\n",
    "            left = int((x - w / 2) * annotated_frame.shape[1])\n",
    "            top = int((y - h / 2) * annotated_frame.shape[0])\n",
    "            right = int((x + w / 2) * annotated_frame.shape[1])\n",
    "            bottom = int((y + h / 2) * annotated_frame.shape[0])\n",
    "            cv2.rectangle(annotated_frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "            cv2.putText(annotated_frame, f'Class: {class_id}, Score: {score:.2f}', \n",
    "                        (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, \n",
    "                        (0, 255, 0), 2)\n",
    "    print(detections.T[0])\n",
    "\n",
    "    # 알고리즘 종료 시점\n",
    "    terminate_t = timeit.default_timer()\n",
    "    \n",
    "    FPS = int(1./(terminate_t - start_t ))\n",
    "    cv2.imshow('video',frame)\n",
    "    print(FPS)\n",
    "\n",
    "    # 화면에 표시\n",
    "    cv2.imshow(\"YOLOv8 Real-Time Detection\", annotated_frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
